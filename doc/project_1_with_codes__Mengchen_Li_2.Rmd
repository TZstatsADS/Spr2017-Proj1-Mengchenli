---
output: pdf_document
---

# "What did the presidents asy at their inauguation?"

Here I explore the texts of U.S.presidents'inaugrual speeches, from that of George Washington to that of Donald Trump which was delivered on 20th Jan. I use tools from text mining and natural language processing like sentiment analysis and topic modeling to analyze interesting trends and patterns.


```{r, message=FALSE, warning=FALSE, echo=FALSE}
# step 0: Install and Load necessary libraries
library("tm")
library("wordcloud")
library("RColorBrewer")
library("dplyr")
library("tidytext")
library("openxlsx")
library("rvest")
library("tibble")
library("qdap")
library("sentimentr")
library("gplots")
library("dplyr")
library("syuzhet")
library("factoextra")
library("beeswarm")
library("scales")
library("RANN")
library("topicmodels")
library("shiny")
```


First, I will create the wordclouds for both overall speech and individual speeches
```{r, message=FALSE, warning=FALSE, echo=FALSE}
# Read in the speech

# Relative path to the local folder
folder.path = '/Users/limengchen/Desktop/Spr2017-Proj1-Mengchenli-master/data/InauguralSpeeches'

# get the list of file names
speeches = list.files(path = folder.path, pattern = '*.txt') 

# file names only with 'Fistlast-Term'
prez.out = substr(speeches, 6, nchar(speeches)-4)
```


```{r, message=FALSE, warning=FALSE, echo=FALSE}
# Text Cleaning

# Create a NA vector which equals the length of the number of speeches
length.speeches = rep(NA, length(speeches))

# Create a corpus
ff.all = Corpus(DirSource(folder.path))

# Strip all spaces to a single space
ff.all<-tm_map(ff.all, stripWhitespace) 

# Convert to lower-case
ff.all<-tm_map(ff.all, content_transformer(tolower))

# Remove filler words
ff.all<-tm_map(ff.all, removeWords, stopwords("english"))

# Remove empty strings
ff.all<-tm_map(ff.all, removeWords, character(0))

# Remove punctuation
ff.all<-tm_map(ff.all, removePunctuation) 

# Make a document-term matrix
tdm.all<-TermDocumentMatrix(ff.all)

# Create a dataframe with three columns: words, document and count
tdm.tidy = tidy(tdm.all)

# Finding and summerizing the most frequent terms 
# Create a tibble with two columns: word and count
tdm.overall=summarise(group_by(tdm.tidy, term), sum(count))
```


# word cloud for overall speech
```{r, message=FALSE, warning=FALSE, echo=FALSE}
# Inspect an overall wordcloud
wordcloud(tdm.overall$term, tdm.overall$`sum(count)`,
          scale=c(5,0.5),
          max.words=100,
          min.freq=1,
          random.order=FALSE,
          rot.per=0.3,
          use.r.layout=T,
          random.color=FALSE,
          colors=brewer.pal(10,"Blues"))
```

I noticed most of the speeches mentioned 'will', 'government' and 'people'. I guess they all expected to strengthen the relationship between government and people. What about the focus of each individual speech? 


```{r, message=FALSE, warning=FALSE, echo=FALSE}
# compute TF-IDF weighted document-term matrices for individual speeches

dtm = DocumentTermMatrix(ff.all,
                          control = list(weighting = function(x)
                                             weightTfIdf(x, 
                                                         normalize =FALSE),
                                         stopwords = TRUE))
ff.dtm=tidy(dtm)
```


# Interactive visualize important words in individual speeches
```{r, essage=FALSE, warning=FALSE, echo=FALSE}
shinyApp(
    ui = fluidPage(
      fluidRow(style = "padding-bottom: 20px;",
        column(4, selectInput('speech1', 'Speech 1',
                              speeches,
                              selected=speeches[5])),
        column(4, selectInput('speech2', 'Speech 2', speeches,
                              selected=speeches[9])),
        column(4, sliderInput('nwords', 'Number of words', 3,
                               min = 20, max = 200, value=100, step = 20))
      ),
      fluidRow(
        plotOutput('wordclouds', height = "400px")
      )
    ),
    server = function(input, output, session) {
      # Combine the selected variables into a new data frame
      selectedData <- reactive({
        list(dtm.term1=ff.dtm$term[ff.dtm$document==as.character(input$speech1)],
             dtm.count1=ff.dtm$count[ff.dtm$document==as.character(input$speech1)],
             dtm.term2=ff.dtm$term[ff.dtm$document==as.character(input$speech2)],
             dtm.count2=ff.dtm$count[ff.dtm$document==as.character(input$speech2)])
      })
      output$wordclouds <- renderPlot(height = 400, {
        par(mfrow=c(1,2), mar = c(0, 0, 3, 0))
        wordcloud(selectedData()$dtm.term1, 
                  selectedData()$dtm.count1,
              scale=c(5,0.5),
              max.words=input$nwords,
              min.freq=1,
              random.order=FALSE,
              rot.per=0,
              use.r.layout=FALSE,
              random.color=FALSE,
              colors=brewer.pal(10,"Blues"), 
            main=input$speech1)
        wordcloud(selectedData()$dtm.term2, 
                  selectedData()$dtm.count2,
              scale=c(5,0.5),
              max.words=input$nwords,
              min.freq=1,
              random.order=FALSE,
              rot.per=0,
              use.r.layout=FALSE,
              random.color=FALSE,
              colors=brewer.pal(10,"Blues"), 
            main=input$speech2)
      })
    },
    options = list(height = 500)
)
```

Let's choose the most recent two speeches between Obama and Trump to compare.
I noticed from the word cloud of Trump that amoung the most frequent words in this inaugrual speeches are 'America', 'obama', 'dreams'. This suggests that the thoughts of Trump is probably associated with Obama which helps American to achieve their dreams in the future. However, Obama emphasized the jobs, realtively different from Trump. 



Next, I use sentences as units of analysis for this project since sentences are natural languge units for organizing thoughts and ideas. For each extracted sentence, I apply sentiment analysis by using NRC sentiment lexion. I use a sequential id for every sentence called 'sent.id' and calulated the number of words in each sentence called 'sentence length'.


```{r,message=FALSE, warning=FALSE, echo=FALSE}
# Scrap the main texts of speeches

setwd("/Users/limengchen/Desktop/data/InauguralSpeeches")
data = read.xlsx("/Users/limengchen/Desktop/Spr2017-Proj1-Mengchenli-master/data/InaugurationInfo.xlsx", sheet = 1)

# Get the president name of speech
ups = unique(data$File)

# Get each speech name
files = paste0("inaug",paste(data$File, data$Term, sep = "-"),".txt")

# Create a data frame for fulltext of all speeches
speech.list = NULL

for(i in 1:length(files)){
  sp = paste(readLines(files[i],n=-1, skipNul=TRUE),collapse=" ")
  speech.list  = c(speech.list,sp)
}
speech.list = data.frame(fulltext = speech.list)
```


```{r, message=FALSE, warning=FALSE, echo=FALSE}
# Read each sentence for all speeches
# Detect and split sentences on endmark boundaries.
sentence.list=NULL
for(i in 1:nrow(speech.list)){
  sentences=sent_detect(speech.list$fulltext[i],
                        endmarks = c("?", ".", "!", "|",";"))

# Calls the NRC sentiment dictionary to calculate the presence of eight different emotions and their corresponding valence in a text file
# Count word numbers for each sentence
  if(length(sentences)>0){
    emotions=get_nrc_sentiment(sentences)
    word.count=word_count(sentences)
    # colnames(emotions)=paste0("emo.", colnames(emotions))
    # in case the word counts are zeros?
    emotions=diag(1/(word.count+0.01))%*%as.matrix(emotions)
    sentence.list=rbind(sentence.list, 
                        cbind(speech.list[i,-ncol(speech.list)],
                              sentences=as.character(sentences), 
                              word.count,
                              emotions,
                              sent.id=1:length(sentences), File = data$File[i],
                              Term = data$Term[i]
                        )
    )
  }
}

# some non-sentences exist in raw data due to erroneous extra end-of sentence marks
sentence.list=
  sentence.list%>%
  filter(!is.na(word.count)) 
```


After generating each sentences for all speeches, I expect to focus on analyzing the length of sentences. I chose a sebset of better known presidents or presidential candidates to analysis for simple visualization.

```{r, message=FALSE, warning=FALSE, echo=FALSE}
# Select several speech to compare

sel.comparison=c("DonaldJTrump","JohnMcCain", "GeorgeBush", "MittRomney", "GeorgeWBush", "RonaldReagan","AlbertGore,Jr","HillaryClinton","JohnFKerry", "WilliamJClinton","HarrySTruman", "BarackObama", "LyndonBJohnson","GeraldRFord", "JimmyCarter", "DwightDEisenhower", "FranklinDRoosevelt","HerbertHoover","JohnFKennedy","RichardNixon","WoodrowWilson","AbrahamLincoln", "TheodoreRoosevelt", "JamesGarfield","JohnQuincyAdams", "UlyssesSGrant", "ThomasJefferson", "GeorgeWashington", "WilliamHowardTaft", "AndrewJackson","WilliamHenryHarrison", "JohnAdams")
```


# Word numbers of sentences for each selectd speech
```{r, message=FALSE, warning=FALSE, echo=FALSE}
# Create a sentence list for selected speech

par(mar=c(4, 11, 2, 2))
pos = which(sentence.list$Term==1 & sentence.list$File%in%sel.comparison)
#sel.comparison=levels(sentence.list$FileOrdered)
sentence.list.sel= sentence.list[pos,]
sentence.list.sel$File= factor(sentence.list.sel$File)

sentence.list.sel$FileOrdered=reorder(sentence.list.sel$File, 
                                      sentence.list.sel$word.count, 
                                      mean, 
                                      order=T)

# Create a bee swarm plot for the word number in each sentence for each selected speech
beeswarm(word.count~FileOrdered, 
         data=sentence.list.sel,
         horizontal = TRUE, 
         pch=16, col=alpha(brewer.pal(9, "Set1"), 0.6), 
         cex=0.55, cex.axis=0.8, cex.lab=0.8,
         spacing=5/nlevels(sentence.list.sel$FileOrdered),
         las=2, xlab="Number of words in a sentence.", ylab="",
         main="Inaugural speeches")


```

Compared with prior prior U.S. presidents such as George Washington, I noticed that several presidents especially for Donoald Trump prefered very short sentences in their speech.
As time goes on, most of the presidents prefer short sentences, which probabely seem more free-style speeches. I guess Trump expected to talk very little but do a lot in the future.


So, What are these short sentences in Trump and Obama speeches? Let's compare the differences.
```{r, message=FALSE, warning=FALSE, echo=FALSE}
sentence.list %>%
  filter(File == 'DonaldJTrump',
         word.count <= 3) %>%
  select(sentences)

sentence.list %>%
  filter(File == 'BarackObama',
         word.count <= 3) %>%
  select(sentences)

```

I noticed Lots of unformal sentences used in his speech such as 'Thank you' and 'God bless America', which probabely proved they weren't good at delivering a formal speech.


What about the emotions in all speeches?
How the presidents shift sentiments in their speeches?
Now, I will cluster the emtions in all speeches

# Clustering of emotions
```{r, message=FALSE, warning=FALSE, echo=FALSE}
# Create a heat map of sentences for all speeches
# Cluster emotions for sentences for all speeches

heatmap.2(cor(sentence.list%>%select(anger:trust)), 
          scale = "none", 
          col = bluered(100),  margin=c(6, 6), key=F,
          trace = "none", density.info = "none")

```

Looking through all speeches, I can cluster emotions into two main groups.
Group1 (negative): anger, fear, sadness, disgust
Group2 (positive): trust, surprise, joy, anticipation
These two groups represents very differrent emotions.


# Bar plot for mean value of clustering emotions for sentence list
```{r, message=FALSE, warning=FALSE, echo=FALSE}
# Compute column means for sentence list
# Create a bar plot for mean value of clustering emotions for sentence list

par(mar=c(4, 6, 2, 1))
emo.means=colMeans(select(sentence.list, anger:trust)>0.01)
col.use=c("red2", "darkgoldenrod1", 
          "chartreuse3", "blueviolet",
          "darkgoldenrod2", "dodgerblue3", 
          "darkgoldenrod1", "darkgoldenrod1")
barplot(emo.means[order(emo.means)], las=2, col=col.use[order(emo.means)], horiz=T, main="Inaugural Speeches")
```

I noticed that most of positive emotions such as trust and anticipation are widely used in speech of presidents. In contrast, they avoided negative emotions such as disgust and sadness.
Although presidents still felt fear and surprise when they delivered speeches, they all expected to contribute their positive emotions to audience so that all people in America would have confidence to support them.


Let's look at Trump and Obama in detail. What are the emotionally charged sentencesin their speeches?
# Sentences with emotions for Trump and Obama
```{r, message=FALSE, warning=FALSE, echo=FALSE}
source("/Users/limengchen/Desktop/Tutorial2/lib/plotstacked.R")
source("/Users/limengchen/Desktop/Tutorial2/lib/speechFuncs.R")

print('Donald Trump')
speech.df = tbl_df(sentence.list) %>%
  filter(File == 'DonaldJTrump', word.count >= 4) %>%
  select(sentences, anger:trust)
speech.df = as.data.frame(speech.df)
as.character(speech.df$sentences[apply(speech.df[,-1], 2, which.max)])

print('Barack Obama')
speech.df = tbl_df(sentence.list) %>%
  filter(File == 'BarackObama', word.count >= 4) %>%
  select(sentences, anger:trust)
speech.df = as.data.frame(speech.df)
as.character(speech.df$sentences[apply(speech.df[,-1], 2, which.max)])
```

For Donald Trump, I notied that all these sentences with emotions expects to deliver the same topic: America will become more powerful in the future.
However, for Barack Obama, he mentioned twice 'Our capacity remains undiminished', which probably shows his fear for capital and jobs.


What about the clustering emotions relationship amoung all speeches?
In this project, I expect to cluster them into five groups. That means presidents in the same group used similar emotions in their speeches.

```{r, message=FALSE, warning=FALSE, echo=FALSE}
# Summarize the mean values of clustering emotions as a data frame
presid.summary=tbl_df(sentence.list)%>%
  subset(File%in%sel.comparison)%>%
  #group_by(paste0(type, File))%>%
  group_by(File)%>%
  summarise(
    anger=mean(anger),
    anticipation=mean(anticipation),
    disgust=mean(disgust),
    fear=mean(fear),
    joy=mean(joy),
    sadness=mean(sadness),
    surprise=mean(surprise),
    trust=mean(trust)
    #negative=mean(negative),
    #positive=mean(positive)
  )
presid.summary=as.data.frame(presid.summary)
rownames(presid.summary)=as.character((presid.summary[,1]))

# Perform k-means clustering on emotions data frame
# Clustering emotions into 5 groups 
# Create ggplot based on clustering emotions
km.res=kmeans(presid.summary[,-1], iter.max=200,
              5)
fviz_cluster(km.res, 
             stand=F, repel= TRUE,
             data = presid.summary[,-1], xlab="", xaxt="n",
             show.clust.cent=FALSE)
```

I see that DonoldJTrump, GeorgeBush, JimmyCarter and JohnAdams are analyzed in the same cluster. Although they used different length of sentences in their speeches, I was surprised that they delivered similar emotions.
Thus, there are no necessary relationships between the length of sentences and the emotion of speeches.


Finally, I expect to dig more stories on the topic of speeches.
# Data analysis -- Topic modeling

```{r, message=FALSE, warning=FALSE, echo=FALSE}
# Create a corpus list for sentences

corpus.list=sentence.list[2:(nrow(sentence.list)-1), ]
sentence.pre=sentence.list$sentences[1:(nrow(sentence.list)-2)]
sentence.post=sentence.list$sentences[3:(nrow(sentence.list)-1)]
corpus.list$snipets=paste(sentence.pre, corpus.list$sentences, sentence.post, sep=" ")
rm.rows=(1:nrow(corpus.list))[corpus.list$sent.id==1]
rm.rows=c(rm.rows, rm.rows-1)
corpus.list=corpus.list[-rm.rows, ]

docs = Corpus(VectorSource(corpus.list$snipets))
```


```{r, message=FALSE, warning=FALSE, echo=FALSE}
# text mining for corpus

#remove potentially problematic symbols
docs = tm_map(docs,content_transformer(tolower))

#remove punctuation
docs = tm_map(docs, removePunctuation)

#Strip digits
docs = tm_map(docs, removeNumbers)

#remove stopwords
docs = tm_map(docs, removeWords, stopwords("english"))

#remove whitespace
docs = tm_map(docs, stripWhitespace)

#Stem document
docs = tm_map(docs,stemDocument)
```


```{r, message=FALSE, warning=FALSE, echo=FALSE}
# Create a document term matrix
dtm <- DocumentTermMatrix(docs)

#convert rownames to filenames
rownames(dtm) <- paste(corpus.list$type, corpus.list$File,
                       corpus.list$Term, corpus.list$sent.id, sep="_")

#Find the sum of words in each Document
rowTotals <- apply(dtm , 1, sum) 

dtm  <- dtm[rowTotals> 0, ]
corpus.list=corpus.list[rowTotals>0, ]
```


# Create 10 topics for all speeches
```{r, message=FALSE, warning=FALSE, echo=FALSE}
# Appy LDA for document term matrix

#Set parameters for Gibbs sampling
burnin <- 4000
iter <- 1000
thin <- 500
seed <-list(2003,5,63,100001,765)
nstart <- 5
best <- TRUE
```


```{r, message=FALSE, warning=FALSE, echo=FALSE}
#Number of topics
k <- 10

#Run LDA using Gibbs sampling
ldaOut <-LDA(dtm, k, method="Gibbs", control=list(nstart=nstart, 
                                                  seed = seed, best=best,
                                                  burnin = burnin, iter = iter, thin=thin))

```


```{r, message=FALSE, warning=FALSE,echo=FALSE}
#write out results
#docs to topics
ldaOut.topics <- as.matrix(topics(ldaOut))
table(c(1:k, ldaOut.topics))

#top 6 terms in each topic
ldaOut.terms <- as.matrix(terms(ldaOut,20))

#probabilities associated with each topic assignment
topicProbabilities <- as.data.frame(ldaOut@gamma)

terms.beta=ldaOut@beta
terms.beta=scale(terms.beta)
topics.terms=NULL
for(i in 1:k){
  topics.terms=rbind(topics.terms, ldaOut@terms[order(terms.beta[i,], decreasing = TRUE)[1:7]])
}
topics.terms
ldaOut.terms
```

In summary, presidents all used relatively different topics and emotions. However, they all expect to try their best to strengthen the power of Americ in the world. Although, what happens in the future is uncertain, I suppose American should support Trump now so that they can achieve their dreams.


