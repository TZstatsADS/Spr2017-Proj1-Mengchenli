---
output:
  pdf_document: default
  html_document: default
---

# What did the presidents say at their inauguation?


Here I explore the texts of U.S.presidents'inaugrual speeches, from that of George Washington to that of Donald Trump which was delivered on 20th Jan. I use tools from text mining and natural language processing like sentiment analysis and topic modeling to analyze interesting trends and patterns.


```{r, message=FALSE, warning=FALSE, echo=FALSE}
# step 0: Install and Load necessary libraries
library("tm")
library("wordcloud")
library("RColorBrewer")
library("dplyr")
library("tidytext")
library("openxlsx")
library("rvest")
library("tibble")
library("qdap")
library("sentimentr")
library("gplots")
library("dplyr")
library("syuzhet")
library("factoextra")
library("beeswarm")
library("scales")
library("RANN")
library("topicmodels")
library("shiny")
library("cluster")
library("ggplot2")
library("magrittr")
library("showtext")
```


First, I will create the wordclouds for both overall speech and individual speeches

```{r, message=FALSE, warning=FALSE,echo=FALSE}
# Read in the speech

# Relative path to the local folder
folder.path = '/Users/limengchen/Desktop/Spr2017-Proj1-Mengchenli-master/data/InauguralSpeeches'

# get the list of file names
speeches = list.files(path = folder.path, pattern = '*.txt') 

# file names only with 'Fistlast-Term'
prez.out = substr(speeches, 6, nchar(speeches)-4)

```

```{r, message=FALSE, warning=FALSE,echo=FALSE}
# Text Cleaning

# Create a NA vector which equals the length of the number of speeches
length.speeches = rep(NA, length(speeches))
# Create a corpus
ff.all = Corpus(DirSource(folder.path))
# Strip all spaces to a single space
ff.all = tm_map(ff.all, stripWhitespace) 
# Convert to lower-case
ff.all = tm_map(ff.all, content_transformer(tolower))
# Remove filler words
ff.all = tm_map(ff.all, removeWords, stopwords("english"))
# Remove empty strings
ff.all = tm_map(ff.all, removeWords, character(0))
# Remove punctuation
ff.all = tm_map(ff.all, removePunctuation) 
# Make a document-term matrix
tdm.all = TermDocumentMatrix(ff.all)

# Create a dataframe with three columns: words, document and count
tdm.tidy = tidy(tdm.all)

# Finding and summerizing the most frequent terms 
# Create a tibble with two columns: word and count
tdm.overall=summarise(group_by(tdm.tidy, term), sum(count))
```

# word cloud for overall speech
```{r, message=FALSE, warning=FALSE,echo=FALSE}
# Inspect an overall wordcloud
wordcloud(tdm.overall$term, tdm.overall$`sum(count)`,
          scale=c(5,0.5),
          max.words=100,
          min.freq=1,
          random.order=FALSE,
          rot.per=0.3,
          use.r.layout=T,
          random.color=FALSE,
          colors=brewer.pal(10,"Blues"),
          title = 'wordcloud for all speeches')
```

I noticed most of the speeches mentioned 'will', 'government' and 'people'. I guess they all expected to strengthen the relationship between government and people. What about focus for different parties?

Now, I will spilt all speeches into two groups: Republican party and Democratic party.

```{r, message=FALSE, warning=FALSE,echo=FALSE}
data = read.xlsx("/Users/limengchen/Desktop/Spr2017-Proj1-Mengchenli-master/data/InaugurationInfo.xlsx", sheet = 1)

# Tokenize the corpus
myCorpusTokenized = lapply(ff.all, scan_tokenizer)
# concatenate tokens by document, create data frame
myDf = data.frame(text = sapply(myCorpusTokenized, paste, collapse = " "), stringsAsFactors = FALSE)
myDf = cbind(data,myDf)
myDf[] = lapply(myDf, as.character)
```

```{r, message=FALSE, warning=FALSE,echo=FALSE}
# Republican party
repub=myDf$text[nrow(myDf)]
for (i in seq(nrow(myDf)))
{
  if(is.na(myDf$Party[i]))
  {
    myDf$Party[i]="No Party"
  }
  
  if((myDf$Party[i]=="Republican"))
  {
    temp=myDf$text[i]  
    repub=rbind(repub,temp)
  }
}
repub.corpus=Corpus(DataframeSource(repub))

# Democratic party
demo=myDf$text[nrow(myDf)]
for (i in seq(nrow(myDf)))
{
  if(is.na(myDf$Party[i]))
  {
    myDf$Party[i]="No Party"
  }
  
  if((myDf$Party[i]=="Democratic"))
  {
    temp=myDf$text[i]  
    demo=rbind(demo,temp)
  }
}
demo.corpus=Corpus(DataframeSource(demo))
```

```{r, message=FALSE, warning=FALSE,echo=FALSE}
# Text mining for two parties

repub = tm_map(repub.corpus, stripWhitespace) 
repub = tm_map(repub.corpus, content_transformer(tolower))
repub = tm_map(repub.corpus, removeWords, stopwords("english"))
repub = tm_map(repub.corpus, removeWords, character(0))
repub = tm_map(repub.corpus, removePunctuation)

demo = tm_map(demo.corpus, stripWhitespace) 
demo = tm_map(demo.corpus, content_transformer(tolower))
demo = tm_map(demo.corpus, removeWords, stopwords("english"))
demo = tm_map(demo.corpus, removeWords, character(0))
demo = tm_map(demo.corpus, removePunctuation)
```

```{r, message=FALSE, warning=FALSE,echo=FALSE}
# Make a document-term matrix
tdm.repub<-TermDocumentMatrix(repub)
tdm.demo<-TermDocumentMatrix(demo)

# Create a dataframe with three columns: words, document and count
tdm.tidy.repub = tidy(tdm.repub)
tdm.tidy.demo = tidy(tdm.demo)

# Finding and summerizing the most frequent terms 
# Create a tibble with two columns: word and count
tdm.overall.repub=summarise(group_by(tdm.tidy.repub, term), sum(count))
tdm.overall.demo=summarise(group_by(tdm.tidy.demo, term), sum(count))
```

# Word cloud for republican party and democratic party
```{r, message=FALSE, warning=FALSE,echo=FALSE}
par(mfrow = c(1,2))

wordcloud(tdm.overall.repub$term, tdm.overall.repub$`sum(count)`,
          scale=c(5,0.5),
          max.words=100,
          min.freq=1,
          random.order=FALSE,
          rot.per=0.3,
          use.r.layout=T,
          random.color=FALSE,
          colors=brewer.pal(10,"Blues"),
          title = 'wordcloud for republican party')

wordcloud(tdm.overall.demo$term, tdm.overall.demo$`sum(count)`,
          scale=c(5,0.5),
          max.words=100,
          min.freq=1,
          random.order=FALSE,
          rot.per=0.3,
          use.r.layout=T,
          random.color=FALSE,
          colors=brewer.pal(10,"Blues"),
          title = 'wordcloud for democratic party')

```

Although most of focusing word are smilar such as 'people', there are still several differences between these two parties. The repuclican party mentioned more about 'country' and 'states' while the democratic party emphasized 'world' and 'nation'. This probably suggested that the republican party had more development plan within country, but the democratic one expected to increase the infulences of America in the world.

By creating the interactive visualize important words in individual speeches before, Let's choose the most typical example for two parties between Obama and Trump to compare.I noticed from the word cloud of Trump that amoung the most frequent words in this inaugrual speeches are 'America', 'obama', 'dreams'. This suggests that the thoughts of Trump is probably associated with Obama which helps American to achieve their dreams in the future. However, Obama emphasized the jobs, realtively different from Trump.

Thus, the focusing words are different when I consider both parties and presidents.



Next, I use sentences as units of analysis for this project since sentences are natural languge units for organizing thoughts and ideas. For each extracted sentence, I apply sentiment analysis by using NRC sentiment lexion. I use a sequential id for every sentence called 'sent.id' and calulated the number of words in each sentence called 'sentence length'.
```{r, message=FALSE, warning=FALSE,echo=FALSE}
# Scrap the main texts of speeches

setwd("/Users/limengchen/Desktop/Spr2017-Proj1-Mengchenli-master/data/InauguralSpeeches")
data = read.xlsx("/Users/limengchen/Desktop/Spr2017-Proj1-Mengchenli-master/data/InaugurationInfo.xlsx", sheet = 1)

# Get the president name of speech
ups = unique(data$File)

# Get each speech name
files = paste0("inaug",paste(data$File, data$Term, sep = "-"),".txt")

# Create a data frame for fulltext of all speeches
speech.list = NULL

for(i in 1:length(files)){
  sp = paste(readLines(files[i],n=-1, skipNul=TRUE),collapse=" ")
  speech.list  = c(speech.list,sp)
}
speech.list = data.frame(fulltext = speech.list)
```

```{r, message=FALSE, warning=FALSE,echo=FALSE}
# Read each sentence for all speeches
# Detect and split sentences on endmark boundaries.
sentence.list=NULL
for(i in 1:nrow(speech.list)){
  sentences=sent_detect(speech.list$fulltext[i],
                        endmarks = c("?", ".", "!", "|",";"))

# Calls the NRC sentiment dictionary to calculate the presence of eight different emotions and their corresponding valence in a text file
# Count word numbers for each sentence
  if(length(sentences)>0){
    emotions=get_nrc_sentiment(sentences)
    word.count=word_count(sentences)
    # colnames(emotions)=paste0("emo.", colnames(emotions))
    # in case the word counts are zeros?
    emotions=diag(1/(word.count+0.01))%*%as.matrix(emotions)
    sentence.list=rbind(sentence.list, 
                        cbind(speech.list[i,-ncol(speech.list)],
                              sentences=as.character(sentences), 
                              word.count,
                              emotions,
                              sent.id=1:length(sentences), File = data$File[i],
                              Term = data$Term[i]
                        )
    )
  }
}

# some non-sentences exist in raw data due to erroneous extra end-of sentence marks
sentence.list=
  sentence.list%>%
  filter(!is.na(word.count))
```

After generating each sentences for all speeches, I expect to focus on analyzing the length of sentences. I chose a sebset of better known presidents or presidential candidates to analysis for simple visualization.
```{r, message=FALSE, warning=FALSE,echo=FALSE}
# Select several speech to compare

sel.comparison=c("DonaldJTrump","JohnMcCain", "GeorgeBush", "MittRomney", "GeorgeWBush", "RonaldReagan","AlbertGore,Jr","HillaryClinton","JohnFKerry", "WilliamJClinton","HarrySTruman", "BarackObama", "LyndonBJohnson","GeraldRFord", "JimmyCarter", "DwightDEisenhower", "FranklinDRoosevelt","HerbertHoover","JohnFKennedy","RichardNixon","WoodrowWilson","AbrahamLincoln", "TheodoreRoosevelt", "JamesGarfield","JohnQuincyAdams", "UlyssesSGrant", "ThomasJefferson", "GeorgeWashington", "WilliamHowardTaft", "AndrewJackson","WilliamHenryHarrison", "JohnAdams")
```

# Word numbers of sentences for each selectd speech
```{r, message=FALSE, warning=FALSE,echo=FALSE}
# Create a sentence list for selected speech

par(mar=c(4, 11, 2, 2))
pos = which(sentence.list$Term==1 & sentence.list$File%in%sel.comparison)
#sel.comparison=levels(sentence.list$FileOrdered)
sentence.list.sel= sentence.list[pos,]
sentence.list.sel$File= factor(sentence.list.sel$File)

sentence.list.sel$FileOrdered=reorder(sentence.list.sel$File, 
                                      sentence.list.sel$word.count, 
                                      mean, 
                                      order=T)

# Create a bee swarm plot for the word number in each sentence for each selected speech
beeswarm(word.count~FileOrdered, 
         data=sentence.list.sel,
         horizontal = TRUE, 
         pch=16, col=alpha(brewer.pal(9, "Set1"), 0.6), 
         cex=0.55, cex.axis=0.8, cex.lab=0.8,
         spacing=5/nlevels(sentence.list.sel$FileOrdered),
         las=2, xlab="Number of words in a sentence.", ylab="",
         main="Inaugural speeches")
```
Compared with prior prior U.S. presidents such as George Washington, I noticed that several presidents especially for Donoald Trump prefered very short sentences in their speech.
As time goes on, most of the presidents prefer short sentences, which probabely seem more free-style speeches. I guess Trump expected to talk very little but do a lot in the future.


So, What are these short sentences in Trump and Obama speeches? Let's compare the differences since they are the typical examples for two parties.
```{r, message=FALSE, warning=FALSE,echo=FALSE}
sentence.list %>%
  filter(File == 'DonaldJTrump',
         word.count <= 3) %>%
  select(sentences)

sentence.list %>%
  filter(File == 'BarackObama',
         word.count <= 3) %>%
  select(sentences)

```
I noticed Lots of unformal sentences used in both their speeches such as 'Thank you' and 'God bless America', which probabely proved they weren't good at delivering a formal speech.



What about the emotions in all speeches?
How the presidents shift sentiments in their speeches?
Now, I will cluster the emtions in all speeches.

# Clustering of emotions for all speeches
```{r, message=FALSE, warning=FALSE,echo=FALSE}
# Create a heat map of sentences for all speeches
# Cluster emotions for sentences for all speeches

heatmap.2(cor(sentence.list%>%select(anger:trust)), 
          scale = "none", 
          col = bluered(100),  margin=c(6, 6), key=F,
          trace = "none", density.info = "none")

```
Looking through all speeches, I can cluster emotions into two main groups.
Group1 (negative): anger, fear, sadness, disgust
Group2 (positive): trust, surprise, joy, anticipation
These two groups represents very differrent emotions.


# Bar plot for mean value of clustering emotions for sentence list
```{r, message=FALSE, warning=FALSE,echo=FALSE}
# Compute column means for sentence list
# Create a bar plot for mean value of clustering emotions for sentence list

par(mar=c(4, 6, 2, 1))
emo.means=colMeans(select(sentence.list, anger:trust)>0.01)
col.use=c("red2", "darkgoldenrod1", 
          "chartreuse3", "blueviolet",
          "darkgoldenrod2", "dodgerblue3", 
          "darkgoldenrod1", "darkgoldenrod1")
barplot(emo.means[order(emo.means)], las=2, col=col.use[order(emo.means)], horiz=T, main="Inaugural Speeches")
```
I noticed that most of positive emotions such as trust and anticipation are widely used in speech of presidents. In contrast, they avoided negative emotions such as disgust and sadness.
Although presidents still felt fear and surprise when they delivered speeches, they all expected to contribute their positive emotions to audience so that all people in America would have confidence to support them.


Let's look at Trump and Obama in detail. What are the emotionally charged sentencesin their speeches?
# Sentences with emotions for Trump and Obama
```{r, message=FALSE, warning=FALSE,echo=FALSE}
speech.df = tbl_df(sentence.list) %>%
  filter(File == 'DonaldJTrump', word.count >= 4) %>%
  select(sentences, anger:trust)
speech.df = as.data.frame(speech.df)
as.character(speech.df$sentences[apply(speech.df[,-1], 2, which.max)])

print('Barack Obama')
speech.df = tbl_df(sentence.list) %>%
  filter(File == 'BarackObama', word.count >= 4) %>%
  select(sentences, anger:trust)
speech.df = as.data.frame(speech.df)
as.character(speech.df$sentences[apply(speech.df[,-1], 2, which.max)])
```
For Donald Trump, I notied that all these sentences with emotions expects to deliver the same topic: America will become more powerful in the future.
However, for Barack Obama, he mentioned twice 'Our capacity remains undiminished', which probably shows his fear for capital and jobs.


What about the clustering emotions relationship amoung all speeches?
In this project, I expect to cluster them into five groups. That means presidents in the same group used similar emotions in their speeches.
```{r, message=FALSE, warning=FALSE,echo=FALSE}
# Summarize the mean values of clustering emotions as a data frame
presid.summary=tbl_df(sentence.list)%>%
  subset(File%in%sel.comparison)%>%
  #group_by(paste0(type, File))%>%
  group_by(File)%>%
  summarise(
    anger=mean(anger),
    anticipation=mean(anticipation),
    disgust=mean(disgust),
    fear=mean(fear),
    joy=mean(joy),
    sadness=mean(sadness),
    surprise=mean(surprise),
    trust=mean(trust)
    #negative=mean(negative),
    #positive=mean(positive)
  )
presid.summary=as.data.frame(presid.summary)
rownames(presid.summary)=as.character((presid.summary[,1]))

# Perform k-means clustering on emotions data frame
# Clustering emotions into 5 groups 
# Create ggplot based on clustering emotions
km.res=kmeans(presid.summary[,-1], iter.max=200,
              5)
fviz_cluster(km.res, 
             stand=F, repel= TRUE,
             data = presid.summary[,-1], xlab="", xaxt="n",
             show.clust.cent=FALSE)
```
I see that DonoldJTrump, GeorgeBush, JimmyCarter and JohnAdams are analyzed in the same cluster. Although they used different length of sentences in their speeches, I was surprised that they delivered similar emotions.
Thus, there are no necessary relationships between the length of sentences and the emotion of speeches.



Finally, I expect to dig more stories on the topic of speeches.
# Data analysis -- Topic modeling
```{r, message=FALSE, warning=FALSE,echo=FALSE}
# Create a corpus list for sentences

corpus.list=sentence.list[2:(nrow(sentence.list)-1), ]
sentence.pre=sentence.list$sentences[1:(nrow(sentence.list)-2)]
sentence.post=sentence.list$sentences[3:(nrow(sentence.list)-1)]
corpus.list$snipets=paste(sentence.pre, corpus.list$sentences, sentence.post, sep=" ")
rm.rows=(1:nrow(corpus.list))[corpus.list$sent.id==1]
rm.rows=c(rm.rows, rm.rows-1)
corpus.list=corpus.list[-rm.rows, ]

docs = Corpus(VectorSource(corpus.list$snipets))
```

```{r, message=FALSE, warning=FALSE,echo=FALSE}
# text mining for corpus

docs = tm_map(docs,content_transformer(tolower))
docs = tm_map(docs, removePunctuation)
docs = tm_map(docs, removeNumbers)
docs = tm_map(docs, removeWords, stopwords("english"))
docs = tm_map(docs, stripWhitespace)
docs = tm_map(docs,stemDocument)
```

```{r, message=FALSE, warning=FALSE,echo=FALSE}
# Create a document term matrix
dtm = DocumentTermMatrix(docs)

#convert rownames to filenames
rownames(dtm) = paste(corpus.list$type, corpus.list$File,
                       corpus.list$Term, corpus.list$sent.id, sep="_")

#Find the sum of words in each Document
rowTotals = apply(dtm , 1, sum) 

dtm  = dtm[rowTotals> 0, ]
corpus.list=corpus.list[rowTotals>0, ]
```

# Create 5 topics for all speeches
```{r, message=FALSE, warning=FALSE,echo=FALSE}
# Appy LDA for document term matrix

#Set parameters for Gibbs sampling
burnin = 2000
iter = 500
thin = 250
seed =list(2003,5,63,100001,765)
nstart = 5
best = TRUE
```

```{r, message=FALSE, warning=FALSE,echo=FALSE}
#Number of topics
k = 5

#Run LDA using Gibbs sampling
ldaOut = LDA(dtm, k, method="Gibbs", control=list(nstart=nstart, 
                                                  seed = seed, best=best,
                                                  burnin = burnin, iter = iter, thin=thin))
```

```{r, message=FALSE, warning=FALSE,echo=FALSE}
#write out results
#docs to topics
ldaOut.topics = as.matrix(topics(ldaOut))
table(c(1:k, ldaOut.topics))

#top 6 terms in each topic
ldaOut.terms = as.matrix(terms(ldaOut,20))

#probabilities associated with each topic assignment
topicProbabilities = as.data.frame(ldaOut@gamma)

terms.beta=ldaOut@beta
terms.beta=scale(terms.beta)
topics.terms=NULL
for(i in 1:k){
  topics.terms=rbind(topics.terms, ldaOut@terms[order(terms.beta[i,], decreasing = TRUE)[1:7]])
}
topics.terms
ldaOut.terms
```

After I dig out 5 topics for all speeches, I expect to find wheather there are significant differences of topics between two parties.
```{r,message=FALSE, warning=FALSE,echo=FALSE}
# Create a data frame for fulltext of speeches
speech.list.rep = data.frame(repub)
speech.list.dem = data.frame(demo)


# Read each sentence for all speeches
# Detect and split sentences on endmark boundaries.
sentence.list.rep=NULL
for(i in 1:nrow(speech.list.rep)){
  sentences=sent_detect(speech.list.rep$text[i],
                        endmarks = c("?", ".", "!", "|",";"))
  # Count word numbers for each sentence
  if(length(sentences)>0){
    word.count=word_count(sentences)
    # in case the word counts are zeros?
    sentence.list.rep=rbind(sentence.list.rep, 
                        cbind(speech.list.rep[i,-ncol(speech.list.rep)],
                              sentences=as.character(sentences), 
                              word.count,
                              sent.id=1:length(sentences), File = data$File[i],
                              Term = data$Term[i]
                        )
    )
  }
}

sentence.list.dem=NULL
for(i in 1:nrow(speech.list.dem)){
  sentences=sent_detect(speech.list.dem$text[i],
                        endmarks = c("?", ".", "!", "|",";"))
  # Count word numbers for each sentence
  if(length(sentences)>0){
    word.count=word_count(sentences)
    # in case the word counts are zeros?
    sentence.list.dem=rbind(sentence.list.dem, 
                            cbind(speech.list.dem[i,-ncol(speech.list.dem)],
                                  sentences=as.character(sentences), 
                                  word.count,
                                  sent.id=1:length(sentences), File = data$File[i],
                                  Term = data$Term[i]
                            )
    )
  }
}
```

```{r, message=FALSE, warning=FALSE,echo=FALSE}
# Create a corpus list for sentences
sentence.list.rep1 = sentence.list.rep[,2]
sentence.list.rep1 =data.frame(sentence.list.rep1)
docs.rep = Corpus(VectorSource(sentence.list.rep1))

sentence.list.dem1 = sentence.list.dem[,2]
sentence.list.dem1 =data.frame(sentence.list.dem1)
docs.dem = Corpus(VectorSource(sentence.list.dem1))


docs.rep = tm_map(docs.rep,content_transformer(tolower))
docs.rep = tm_map(docs.rep, removePunctuation)
docs.rep = tm_map(docs.rep, removeNumbers)
docs.rep = tm_map(docs.rep, removeWords, stopwords("english"))
docs.rep = tm_map(docs.rep, stripWhitespace)
docs.rep = tm_map(docs.rep,stemDocument)

docs.dem = tm_map(docs.dem,content_transformer(tolower))
docs.dem = tm_map(docs.dem, removePunctuation)
docs.dem = tm_map(docs.dem, removeNumbers)
docs.dem = tm_map(docs.dem, removeWords, stopwords("english"))
docs.dem = tm_map(docs.dem, stripWhitespace)
docs.dem = tm_map(docs.dem,stemDocument)


# Create a document term matrix
dtm.rep = DocumentTermMatrix(docs.rep)

dtm.dem = DocumentTermMatrix(docs.dem)

#Find the sum of words in each Document
rowTotals.rep = apply(dtm.rep , 1, sum) 
rowTotals.dem = apply(dtm.dem , 1, sum) 

dtm.rep  = dtm.rep[rowTotals> 0, ]
dtm.dem  = dtm.rep[rowTotals> 0, ]
```

```{r, message=FALSE, warning=FALSE,echo=FALSE}
# Appy LDA for document term matrix

#Run LDA using Gibbs sampling
ldaOut.rep <-LDA(dtm.rep, k, method="Gibbs", control=list(nstart=nstart,
                                                              seed = seed, best=best,
                                                              burnin = burnin, iter = iter, thin=thin))
ldaOut.rep

ldaOut.dem <-LDA(dtm.dem, k, method="Gibbs", control=list(nstart=nstart,
                                                              seed = seed, best=best,
                                                              burnin = burnin, iter = iter, thin=thin))
ldaOut.dem

```

```{r, message=FALSE, warning=FALSE,echo=FALSE}
#write out results
#docs to topics
ldaOut.topics.rep <- as.matrix(topics(ldaOut.rep))
table(c(1:k, ldaOut.topics.rep))

ldaOut.topics.dem <- as.matrix(topics(ldaOut.dem))
table(c(1:k, ldaOut.topics.dem))

#top 6 terms in each topic
ldaOut.terms.rep <- as.matrix(terms(ldaOut.rep,20))
ldaOut.terms.dem <- as.matrix(terms(ldaOut.dem,20))

#probabilities associated with each topic assignment
topicProbabilities.rep <- as.data.frame(ldaOut.rep@gamma)
topicProbabilities.dem <- as.data.frame(ldaOut.dem@gamma)

terms.beta.rep=ldaOut.rep@beta
terms.beta.rep=scale(terms.beta.rep)
topics.terms.rep=NULL
for(i in 1:k){
  topics.terms.rep=rbind(topics.terms.rep, ldaOut.rep@terms[order(terms.beta.rep[i,], decreasing = TRUE)[1:7]])
}
topics.terms.rep
ldaOut.terms.rep


terms.beta.dem=ldaOut.dem@beta
terms.beta.dem=scale(terms.beta.dem)
topics.terms.dem=NULL
for(i in 1:k){
  topics.terms.dem=rbind(topics.terms.dem, ldaOut.rep@terms[order(terms.beta.dem[i,], decreasing = TRUE)[1:7]])
}
topics.terms.dem
ldaOut.terms.dem
```
I am very surprised that the topics for two parties are the same. However, compared with the topics of all speeches, the topics for party are different. That means the topics of all speeches are more generalized like looking at the whole country while the thoes of parties are more specific like detailed plan for future.



In summary, presidents all used relatively different topics and emotions. However, they all expect to try their best to strengthen the power of Americ in the world. Although, what happens in the future is uncertain, I suppose American should support Trump now so that they can achieve their dreams.

